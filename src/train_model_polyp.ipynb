{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset and training an image classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Study\\\\Conda Projects\\\\Polyp Detection\\\\TrainingSet_NewGT\\\\train_1k'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining the paths\n",
    "\n",
    "# path_data = os.path.join(os.path.dirname(os.getcwd()),\"Data\")\n",
    "path_images = \"D:\\\\Study\\\\Conda Projects\\\\Polyp Detection\\\\TrainingSet_NewGT\\\\train_1k\"\n",
    "path_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Study\\\\Conda Projects\\\\Polyp Detection\\\\TrainingSet_NewGT\\\\annotations_polyp.csv'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_labels = \"D:\\\\Study\\\\Conda Projects\\\\Polyp Detection\\\\TrainingSet_NewGT\\\\annotations_polyp.csv\"\n",
    "path_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_test_labels = os.path.join(path_data,\"annotations_test.csv\")\n",
    "# path_test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset using custom loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "\n",
    "class polypDataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.labels = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.to_list()\n",
    "        \n",
    "        img_name = os.path.join(self.root_dir,self.labels.iloc[index,0])\n",
    "        img_name = img_name +\".png\"\n",
    "        image = cv2.imread(img_name)\n",
    "        label = self.labels.iloc[index,1]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the transform\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224,224))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataset\n",
    "dataset_cd = polypDataset(csv_file=path_labels, root_dir=path_images, transform=transform)\n",
    "dataloader_cd = DataLoader(dataset_cd, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_test = dogCatDataset(csv_file=path_test_labels, root_dir=path_images, transform=transform)\n",
    "# dataloader_test = DataLoader(dataset_test, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2979"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_cd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([3, 224, 224])\n",
      "1 torch.Size([3, 224, 224])\n",
      "2 torch.Size([3, 224, 224])\n",
      "3 torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for i, sample in enumerate(dataset_cd):\n",
    "    print(i, sample[0].size())\n",
    "\n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Study\\Conda Projects\\opdl\\opdl\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Study\\Conda Projects\\opdl\\opdl\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Changing the final layer based on the number of classes\n",
    "model.fc = nn.Linear(model.fc.in_features,1)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specity the loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/94 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [02:17<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10],Accuracy:0.9933, Loss: 0.0220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [01:46<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10],Accuracy:1.0000, Loss: 0.0013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [02:45<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10],Accuracy:0.9997, Loss: 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [02:57<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10],Accuracy:1.0000, Loss: 0.0024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [03:05<00:00,  1.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10],Accuracy:1.0000, Loss: 0.0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [03:16<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10],Accuracy:0.9997, Loss: 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [03:24<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10],Accuracy:1.0000, Loss: 0.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [03:29<00:00,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10],Accuracy:1.0000, Loss: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [03:32<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10],Accuracy:1.0000, Loss: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [03:30<00:00,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10],Accuracy:0.9997, Loss: 0.0025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):   \n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0.0\n",
    "    total_samples = 0.0\n",
    "    for images, labels in tqdm(dataloader_cd):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        labels = labels.unsqueeze(1)\n",
    "        labels = labels.float()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        predicted_labels = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        correct_predictions += (predicted_labels == labels).sum().item()\n",
    "        total_samples+=labels.size(0)\n",
    "    # auc_score = running_auc/len(dataset_cd)\n",
    "    epoch_loss = running_loss / len(dataset_cd)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    print('Epoch [{}/{}],Accuracy:{:.4f}, Loss: {:.4f}'.format(epoch+1, num_epochs,accuracy, epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Using the model for inference\n",
    "\n",
    "# model = model.eval()\n",
    "# losses = []\n",
    "# correct_predictions = 0\n",
    "# with torch.no_grad():\n",
    "#     for inputs, labels in tqdm(dataloader_test):\n",
    "#         inputs = inputs.to(device)\n",
    "#         labels = labels.to(device)\n",
    "#         outputs = model(inputs)\n",
    "#         labels -= 1\n",
    "#         labels = labels.unsqueeze(1)\n",
    "#         labels = labels.float()\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         # correct_predictions += torch.sum(outputs == labels)\n",
    "#         losses.append(loss.item())\n",
    "#         predicted_labels = (torch.sigmoid(outputs) > 0.5).squeeze(1).long()\n",
    "#         correct_predictions += (predicted_labels == labels).sum().item()\n",
    "# print(correct_predictions / len(dataset_test), np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67843\n",
      "3669\n"
     ]
    }
   ],
   "source": [
    "print(correct_predictions)\n",
    "print(len(dataset_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencing one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test_image = os.path.join(path_images,\"pomeranian_3.jpg\")\n",
    "# path_test_image = os.path.join(path_images,\"Bombay_74.jpg\")\n",
    "test_image = cv2.imread(path_test_image)\n",
    "test_image = transform(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "test_image = test_image.unsqueeze(0)\n",
    "test_image = test_image.to(device)\n",
    "test_image.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_test = model(test_image)\n",
    "predicted_label_test = (torch.sigmoid(output_test) > 0.5).squeeze(1).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1], device='cuda:0')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_label_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),os.path.join(os.path.dirname(os.getcwd()),\"checkpoints\\\\model_polyp_resnet50_epochs10.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grad-CAM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layers = [model.layer4[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = GradCAM(model=model, target_layers=target_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test_image = \"D:\\\\Study\\\\Conda Projects\\\\Polyp Detection\\\\TrainingSet_NewGT\\\\train_1k\\\\ShortVD_wp_4_frame_158_GT.png\"\n",
    "test_image = cv2.imread(path_test_image)\n",
    "test_image = transform(test_image)\n",
    "test_image = test_image.unsqueeze(0)\n",
    "test_image = test_image.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "grayscale_cam = cam(input_tensor=test_image,eigen_smooth=True,aug_smooth=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grayscale_cam_img = grayscale_cam[0, :]\n",
    "grayscale_cam_img = np.where(grayscale_cam_img>0.58,255,0)\n",
    "np.max(grayscale_cam_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(grayscale_cam_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(grayscale_cam_img,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image_cam = cv2.imread(path_test_image)\n",
    "test_image_cam = cv2.resize(test_image_cam,(224,224))\n",
    "test_image_cam = (test_image_cam/255).astype(np.uint8)\n",
    "test_image_cam.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\colormap.cpp:736: error: (-5:Bad argument) cv::ColorMap only supports source images of type CV_8UC1 or CV_8UC3 in function 'cv::colormap::ColorMap::operator ()'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m visualization \u001b[38;5;241m=\u001b[39m \u001b[43mshow_cam_on_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_image_cam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrayscale_cam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_rgb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m Image\u001b[38;5;241m.\u001b[39mfromarray(visualization)\n",
      "File \u001b[1;32md:\\Study\\Conda Projects\\opdl\\opdl\\Lib\\site-packages\\pytorch_grad_cam\\utils\\image.py:48\u001b[0m, in \u001b[0;36mshow_cam_on_image\u001b[1;34m(img, mask, use_rgb, colormap, image_weight)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow_cam_on_image\u001b[39m(img: np\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[0;32m     34\u001b[0m                       mask: np\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[0;32m     35\u001b[0m                       use_rgb: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     36\u001b[0m                       colormap: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mCOLORMAP_JET,\n\u001b[0;32m     37\u001b[0m                       image_weight: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m     38\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" This function overlays the cam mask on the image as an heatmap.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    By default the heatmap is in BGR format.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03m    :returns: The default image with the cam overlay.\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m     heatmap \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapplyColorMap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolormap\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_rgb:\n\u001b[0;32m     50\u001b[0m         heatmap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(heatmap, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\colormap.cpp:736: error: (-5:Bad argument) cv::ColorMap only supports source images of type CV_8UC1 or CV_8UC3 in function 'cv::colormap::ColorMap::operator ()'\n"
     ]
    }
   ],
   "source": [
    "visualization = show_cam_on_image(test_image_cam, grayscale_cam, use_rgb=True)\n",
    "Image.fromarray(visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
